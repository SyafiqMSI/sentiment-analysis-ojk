{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Necessary Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "import nltk\n",
    "os.makedirs(r'C:\\\\nltk_data', exist_ok=True)\n",
    "nltk.data.path.append(r'C:\\\\nltk_data')  \n",
    "nltk.download('punkt_tab', download_dir=r'C:\\\\nltk_data')\n",
    "nltk.download('stopwords', download_dir=r'C:\\\\nltk_data')\n",
    "nltk.download('wordnet', download_dir=r'C:\\\\nltk_data')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_database_result_csv(file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, sep=';', encoding='utf-8')\n",
    "    except UnicodeDecodeError as e1:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, sep=';', encoding='latin1')\n",
    "        except UnicodeDecodeError as e2:\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, sep=';', encoding='cp1252')\n",
    "            except Exception as e3:\n",
    "                print(f\"Gagal membaca file {file_path}: {e1} | {e2} | {e3}\")\n",
    "                return pd.DataFrame()  \n",
    "    return df\n",
    "\n",
    "file_path = 'data/main_data_19.csv'\n",
    "database_result_df = read_database_result_csv(file_path)\n",
    "\n",
    "print(database_result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Necessary Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_order = [\n",
    "    'ID',\n",
    "    'BIDANG',\n",
    "    'SATKER (AKRONIM)',\n",
    "    'JENIS SURVEI',\n",
    "    'TIPE QUESTION',\n",
    "    'INSTITUSI / PERSEORANGAN/ASAL SATKER',\n",
    "    'RESPOND',\n",
    "    'LINK SURVEYMONKEY',\n",
    "    'TOKEN',\n",
    "    'NAMA PIC/RESPONDEN',\n",
    "    'JABATAN/PROFESI/LVEL DI OJK',\n",
    "    'KONTAK',\n",
    "    'FUNGSI YANG DINILAI',\n",
    "    'DIRECT / INDIRECT',\n",
    "    'JENIS STAKEHOLDERS',\n",
    "    'RELASI RESPONDEN DENGAN SATKER',\n",
    "    'POWER',\n",
    "    'INTEREST',\n",
    "    'KATEGORI',\n",
    "    'Dataset',\n",
    "    'RESOURCE PERCEPTION',\n",
    "    'PERFORMANCE DELIVERY',\n",
    "    'OPEN QUESTION 1',\n",
    "    'OPEN QUESTION 2'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_idi_df = database_result_df[columns_order]\n",
    "print(all_data_idi_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_data_idi_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_idi_df.fillna(\"-\", inplace=True)\n",
    "print(all_data_idi_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_idi_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+|\\@\\w+|\\#|\\d+|[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    words = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('indonesian'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    cleaned_text = ' '.join(words)\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Dataset with Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"indobenchmark/indobert-base-p2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=6  \n",
    ")\n",
    "\n",
    "label_map = {\n",
    "    0: \"sangat tidak setuju\",\n",
    "    1: \"tidak setuju\",\n",
    "    2: \"kurang setuju\",\n",
    "    3: \"cukup setuju\",\n",
    "    4: \"setuju\",\n",
    "    5: \"sangat setuju\"\n",
    "}\n",
    "\n",
    "def preprocess_text(text):\n",
    "    return text.strip().lower() if isinstance(text, str) else \"\"  \n",
    "\n",
    "def predict_sentiment(texts):\n",
    "    model.eval()\n",
    "    results = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts):\n",
    "            text = preprocess_text(text)\n",
    "            \n",
    "            if not text:  \n",
    "                results.append({\n",
    "                    'text': text,\n",
    "                    'sentiment': \"unknown\",\n",
    "                    'confidence': 0.0\n",
    "                })\n",
    "                continue\n",
    "            \n",
    "            encoded = tokenizer(\n",
    "                text,\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                max_length=512,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            outputs = model(encoded[\"input_ids\"], attention_mask=encoded[\"attention_mask\"])\n",
    "            predictions = F.softmax(outputs.logits, dim=1)\n",
    "            predicted_label = torch.argmax(predictions, dim=1).item()\n",
    "            \n",
    "            confidence = predictions[0][predicted_label].item()\n",
    "            \n",
    "            sentiment = label_map.get(predicted_label, \"unknown\")\n",
    "            \n",
    "            results.append({\n",
    "                'text': text,\n",
    "                'sentiment': sentiment,\n",
    "                'confidence': confidence\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "columns_to_process = ['OPEN QUESTION 1','OPEN QUESTION 2']\n",
    "\n",
    "all_data_idi_df['Text'] = all_data_idi_df[columns_to_process].fillna(\"\").apply(lambda row: \" \".join(row), axis=1)\n",
    "\n",
    "results = predict_sentiment(all_data_idi_df['Text'].tolist())\n",
    "\n",
    "all_data_idi_df.loc[:, 'Label'] = results['sentiment']\n",
    "\n",
    "print(\"\\nSample of labeled data:\")\n",
    "print(all_data_idi_df[['OPEN QUESTION 1','OPEN QUESTION 2','Label']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_idi_df.to_csv('data/hasil/main_data_19_OQ2.csv', index=False, sep=';')\n",
    "print(all_data_idi_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = all_data_idi_df['Label'].value_counts()\n",
    "\n",
    "label_summary = pd.DataFrame(label_counts).reset_index()\n",
    "label_summary.columns = ['Label', 'Count']\n",
    "print(label_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = all_data_idi_df['Text']\n",
    "y = all_data_idi_df['Label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
