{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to C:\\\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import nltk\n",
    "os.makedirs(r'C:\\\\nltk_data', exist_ok=True)\n",
    "nltk.data.path.append(r'C:\\\\nltk_data')  \n",
    "nltk.download('punkt_tab', download_dir=r'C:\\\\nltk_data')\n",
    "nltk.download('stopwords', download_dir=r'C:\\\\nltk_data')\n",
    "nltk.download('wordnet', download_dir=r'C:\\\\nltk_data')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Responden     Jenis                                   Kualitas Layanan  \\\n",
      "0  Responden 1  Asosiasi  OJK memiliki layanan syariah yang mana semua p...   \n",
      "1  Responden 2  Asosiasi  asosiasi sangat puas atas layanan dan dukungan...   \n",
      "2  Responden 3  Asosiasi  Meski perlu ada peningkatan koordinasi dan kom...   \n",
      "3  Responden 4  Asosiasi                                   Sudah cukup baik   \n",
      "4  Responden 5  Asosiasi                          Masih perlu ditingkatkan.   \n",
      "\n",
      "                                 Hubungan dengan OJK  \\\n",
      "0  Hubungan yang ada mampu menjadi dorongan penti...   \n",
      "1  Setelah pendirian ITSK proses pendirian perusa...   \n",
      "2  SPRINT sudah mumpuni dan memberikan pelayanan ...   \n",
      "3  inovasi layanan masih perlu disediakan untuk m...   \n",
      "4                                        Sandbox 2.0   \n",
      "\n",
      "                                        Kualitas SDM  \\\n",
      "0  SDM di OJK sudah kompeten dan memiliki kemampu...   \n",
      "1  Kompetensi pegawai OJK sudah sangat bagus dan ...   \n",
      "2       OJK sudah canggih dengan SDM yang sudah baik   \n",
      "3  yang masih menjadi masalah adalah struktur org...   \n",
      "4                           Birokrasinya menyulitkan   \n",
      "\n",
      "                                               Saran  \n",
      "0  ke depan perlu adanya kelonggaran dalam regula...  \n",
      "1  Saran kami agar OJK dapat membentuk semacam ta...  \n",
      "2  OJK dapat membentuk regulasi untuk membantu su...  \n",
      "3  memberikan aturan payung untuk mengatur proses...  \n",
      "4  perlu peraturan spesifik untuk sebuah model bi...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def read_database_result_csv(file_path):\n",
    "    try:\n",
    "        # Mencoba membaca dengan encoding default (utf-8) dan delimiter koma\n",
    "        df = pd.read_csv(file_path, sep=';', encoding='utf-8')\n",
    "    except UnicodeDecodeError as e1:\n",
    "        try:\n",
    "            # Jika gagal, mencoba dengan encoding latin1\n",
    "            df = pd.read_csv(file_path, sep=';', encoding='latin1')\n",
    "        except UnicodeDecodeError as e2:\n",
    "            try:\n",
    "                # Jika gagal, mencoba dengan encoding cp1252\n",
    "                df = pd.read_csv(file_path, sep=';', encoding='cp1252')\n",
    "            except Exception as e3:\n",
    "                print(f\"Gagal membaca file {file_path}: {e1} | {e2} | {e3}\")\n",
    "                return pd.DataFrame()  # Mengembalikan DataFrame kosong jika gagal\n",
    "    return df\n",
    "\n",
    "# Path ke file DATABASE_RESULT_OJK.csv\n",
    "file_path = 'data/IDI.csv'\n",
    "database_result_df = read_database_result_csv(file_path)\n",
    "\n",
    "print(database_result_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #FILTERING SATKER\n",
    "\n",
    "# # Menentukan filter SATKER yang diinginkan\n",
    "# filter_option = input(\"Apakah Anda ingin memilih SATKER berdasarkan pilihan tertentu (y/n)? \")\n",
    "\n",
    "# if filter_option.lower() == 'y':\n",
    "#     # Jika pengguna memilih untuk memasukkan SATKER\n",
    "#     selected_satker = input(\"Masukkan SATKER yang ingin difilter (misal: 'DPEA'): \")\n",
    "    \n",
    "#     # Memfilter data berdasarkan SATKER yang dipilih\n",
    "#     filtered_df = database_result_df[database_result_df['SATKER (AKRONIM)'] == selected_satker]\n",
    "#     print(f\"Data difilter berdasarkan SATKER: {selected_satker}\")\n",
    "    \n",
    "# elif filter_option.lower() == 'n':\n",
    "#     # Jika pengguna memilih untuk memfilter berdasarkan beberapa SATKER tertentu\n",
    "#     selected_satker_list = ['DPEA', 'DOSB', 'DPSI']  # Contoh daftar SATKER yang bisa dipilih\n",
    "#     filtered_df = database_result_df[database_result_df['SATKER (AKRONIM)'].isin(selected_satker_list)]\n",
    "#     print(f\"Data difilter berdasarkan beberapa SATKER: {', '.join(selected_satker_list)}\")\n",
    "    \n",
    "# else:\n",
    "#     # Jika input selain 'y' atau 'n', tampilkan pesan\n",
    "#     print(\"Pilihan tidak valid. Menggunakan data tanpa filter.\")\n",
    "#     filtered_df = database_result_df\n",
    "\n",
    "# # Menampilkan data yang sudah difilter\n",
    "# print(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(database_result_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_order = [\n",
    "    'Responden', 'Jenis', 'Kualitas Layanan', 'Hubungan dengan OJK', 'Kualitas SDM', 'Saran'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_idi_df = database_result_df[columns_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            Responden      Jenis  \\\n",
      "0                         Responden 1   Asosiasi   \n",
      "1                         Responden 2   Asosiasi   \n",
      "2                         Responden 3   Asosiasi   \n",
      "3                         Responden 4   Asosiasi   \n",
      "4                         Responden 5   Asosiasi   \n",
      "5                         Responden 6   Asosiasi   \n",
      "6                         Responden 7   Asosiasi   \n",
      "7                         Responden 8   Asosiasi   \n",
      "8                         Responden 9   Asosiasi   \n",
      "9             Responden 10 (Dr. Muiz)  Akademisi   \n",
      "10            Responden 11(Dr. Wendy)  Akademisi   \n",
      "11      Responden 12(Prof. Irwan Adi)  Akademisi   \n",
      "12  Responden 13(Prof. Mamduh Hanafi)  Akademisi   \n",
      "\n",
      "                                     Kualitas Layanan  \\\n",
      "0   OJK memiliki layanan syariah yang mana semua p...   \n",
      "1   asosiasi sangat puas atas layanan dan dukungan...   \n",
      "2   Meski perlu ada peningkatan koordinasi dan kom...   \n",
      "3                                    Sudah cukup baik   \n",
      "4                           Masih perlu ditingkatkan.   \n",
      "5                                    Sudah cukup baik   \n",
      "6                                          cukup baik   \n",
      "7                                          cukup baik   \n",
      "8                                          cukup baik   \n",
      "9   Layanan yang diberikan sudah sesuai, hal ini d...   \n",
      "10  Kinerja Otoritas Jasa Keuangan (OJK) dapat dik...   \n",
      "11  Kinerja Otoritas Jasa Keuangan (OJK) dapat dik...   \n",
      "12  Kesannya sekarang kurang agresif dalam menggap...   \n",
      "\n",
      "                                  Hubungan dengan OJK  \\\n",
      "0   Hubungan yang ada mampu menjadi dorongan penti...   \n",
      "1   Setelah pendirian ITSK proses pendirian perusa...   \n",
      "2   SPRINT sudah mumpuni dan memberikan pelayanan ...   \n",
      "3   inovasi layanan masih perlu disediakan untuk m...   \n",
      "4                                         Sandbox 2.0   \n",
      "5   OJK sudah memberikan roadmap pengembangan dan ...   \n",
      "6   sangat memberikan ruang inovasi untuk pembiaya...   \n",
      "7                                            Sudah OK   \n",
      "8                                          cukup baik   \n",
      "9   Diharapkan portal akademik khusus untuk memfas...   \n",
      "10  Universitas Tanjungpura memiliki hubungan baik...   \n",
      "11                                                  -   \n",
      "12   Interaksi dengan perguruan tinggi sudah sanga...   \n",
      "\n",
      "                                         Kualitas SDM  \\\n",
      "0   SDM di OJK sudah kompeten dan memiliki kemampu...   \n",
      "1   Kompetensi pegawai OJK sudah sangat bagus dan ...   \n",
      "2        OJK sudah canggih dengan SDM yang sudah baik   \n",
      "3   yang masih menjadi masalah adalah struktur org...   \n",
      "4                            Birokrasinya menyulitkan   \n",
      "5   Perlu dikembangkan kapasitas dan edukasi liter...   \n",
      "6   Masih kurang jelas, terkadang industri diminta...   \n",
      "7   Masih banyak sdm di OJK daerah yang belum meng...   \n",
      "8   OJK daerah kurang responsif terhadap laporan i...   \n",
      "9   OJK telah menunjukkan kualitas dan kompetensi ...   \n",
      "10  Kompetensi SDM di pusat sangat baik, terutama ...   \n",
      "11  SDM OJK dinilai berpengalaman dan relevan dala...   \n",
      "12  SDM OJK perlu ditingkatkan dengan benchmarking...   \n",
      "\n",
      "                                                Saran  \n",
      "0   ke depan perlu adanya kelonggaran dalam regula...  \n",
      "1   Saran kami agar OJK dapat membentuk semacam ta...  \n",
      "2   OJK dapat membentuk regulasi untuk membantu su...  \n",
      "3   memberikan aturan payung untuk mengatur proses...  \n",
      "4   perlu peraturan spesifik untuk sebuah model bi...  \n",
      "5   Diberikan kepastian lamanya waktu untuk mendap...  \n",
      "6            Memudahkan birokrasi dan peningkatan SDM  \n",
      "7                          Peningkatan kompetenis sdm  \n",
      "8   Meningkatkan kapasitas sdm khususnya untuk mod...  \n",
      "9                                                   -  \n",
      "10                                                  -  \n",
      "11  Edukasi terhadap masyarakat secara global perl...  \n",
      "12  Saran untuk dibuat forum untuk menunjang akade...  \n"
     ]
    }
   ],
   "source": [
    "print(all_data_idi_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Responden              0\n",
      "Jenis                  0\n",
      "Kualitas Layanan       0\n",
      "Hubungan dengan OJK    0\n",
      "Kualitas SDM           0\n",
      "Saran                  0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(all_data_idi_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Responden</th>\n",
       "      <th>Jenis</th>\n",
       "      <th>Kualitas Layanan</th>\n",
       "      <th>Hubungan dengan OJK</th>\n",
       "      <th>Kualitas SDM</th>\n",
       "      <th>Saran</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Responden 1</td>\n",
       "      <td>Asosiasi</td>\n",
       "      <td>cukup baik</td>\n",
       "      <td>Hubungan yang ada mampu menjadi dorongan penti...</td>\n",
       "      <td>SDM di OJK sudah kompeten dan memiliki kemampu...</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Responden     Jenis Kualitas Layanan  \\\n",
       "count            13        13               13   \n",
       "unique           13         2                9   \n",
       "top     Responden 1  Asosiasi       cukup baik   \n",
       "freq              1         9                3   \n",
       "\n",
       "                                      Hubungan dengan OJK  \\\n",
       "count                                                  13   \n",
       "unique                                                 13   \n",
       "top     Hubungan yang ada mampu menjadi dorongan penti...   \n",
       "freq                                                    1   \n",
       "\n",
       "                                             Kualitas SDM Saran  \n",
       "count                                                  13    13  \n",
       "unique                                                 13    12  \n",
       "top     SDM di OJK sudah kompeten dan memiliki kemampu...     -  \n",
       "freq                                                    1     2  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_idi_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+|\\@\\w+|\\#|\\d+|[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    words = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('indonesian'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    cleaned_text = ' '.join(words)\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 13/13 [00:07<00:00,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of labeled data:\n",
      "                                    Kualitas Layanan  \\\n",
      "0  OJK memiliki layanan syariah yang mana semua p...   \n",
      "1  asosiasi sangat puas atas layanan dan dukungan...   \n",
      "2  Meski perlu ada peningkatan koordinasi dan kom...   \n",
      "3                                   Sudah cukup baik   \n",
      "4                          Masih perlu ditingkatkan.   \n",
      "\n",
      "                                 Hubungan dengan OJK  \\\n",
      "0  Hubungan yang ada mampu menjadi dorongan penti...   \n",
      "1  Setelah pendirian ITSK proses pendirian perusa...   \n",
      "2  SPRINT sudah mumpuni dan memberikan pelayanan ...   \n",
      "3  inovasi layanan masih perlu disediakan untuk m...   \n",
      "4                                        Sandbox 2.0   \n",
      "\n",
      "                                        Kualitas SDM  \\\n",
      "0  SDM di OJK sudah kompeten dan memiliki kemampu...   \n",
      "1  Kompetensi pegawai OJK sudah sangat bagus dan ...   \n",
      "2       OJK sudah canggih dengan SDM yang sudah baik   \n",
      "3  yang masih menjadi masalah adalah struktur org...   \n",
      "4                           Birokrasinya menyulitkan   \n",
      "\n",
      "                                               Saran         Label  Confidence  \n",
      "0  ke depan perlu adanya kelonggaran dalam regula...  cukup setuju    0.255784  \n",
      "1  Saran kami agar OJK dapat membentuk semacam ta...  cukup setuju    0.248169  \n",
      "2  OJK dapat membentuk regulasi untuk membantu su...  cukup setuju    0.271426  \n",
      "3  memberikan aturan payung untuk mengatur proses...  cukup setuju    0.228766  \n",
      "4  perlu peraturan spesifik untuk sebuah model bi...  cukup setuju    0.224682  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ganti model_name dengan model IndoBERT\n",
    "model_name = \"indobenchmark/indobert-base-p1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=6  # Sesuaikan jumlah label\n",
    ")\n",
    "\n",
    "label_map = {\n",
    "    0: \"sangat tidak setuju\",\n",
    "    1: \"tidak setuju\",\n",
    "    2: \"kurang setuju\",\n",
    "    3: \"cukup setuju\",\n",
    "    4: \"setuju\",\n",
    "    5: \"sangat setuju\"\n",
    "}\n",
    "\n",
    "# Fungsi untuk memproses teks\n",
    "def preprocess_text(text):\n",
    "    return text.strip().lower() if isinstance(text, str) else \"\"  # Periksa apakah teks valid\n",
    "\n",
    "# Fungsi untuk prediksi sentimen\n",
    "def predict_sentiment(texts):\n",
    "    model.eval()\n",
    "    results = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts):\n",
    "            text = preprocess_text(text)\n",
    "            \n",
    "            if not text:  # Lewati teks kosong\n",
    "                results.append({\n",
    "                    'text': text,\n",
    "                    'sentiment': \"unknown\",\n",
    "                    'confidence': 0.0\n",
    "                })\n",
    "                continue\n",
    "            \n",
    "            # Tokenisasi teks\n",
    "            encoded = tokenizer(\n",
    "                text,\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                max_length=512,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            # Model memprediksi\n",
    "            outputs = model(encoded[\"input_ids\"], attention_mask=encoded[\"attention_mask\"])\n",
    "            predictions = F.softmax(outputs.logits, dim=1)\n",
    "            predicted_label = torch.argmax(predictions, dim=1).item()\n",
    "            \n",
    "            # Ambil confidence score\n",
    "            confidence = predictions[0][predicted_label].item()\n",
    "            \n",
    "            # Mapping hasil prediksi ke label\n",
    "            sentiment = label_map.get(predicted_label, \"unknown\")\n",
    "            \n",
    "            # Menambahkan hasil ke dalam list\n",
    "            results.append({\n",
    "                'text': text,\n",
    "                'sentiment': sentiment,\n",
    "                'confidence': confidence\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Gabungkan teks dari kolom-kolom yang diinginkan\n",
    "columns_to_process = ['Kualitas Layanan', 'Hubungan dengan OJK', 'Kualitas SDM', 'Saran']\n",
    "all_data_idi_df['Combined_Text'] = all_data_idi_df[columns_to_process].fillna(\"\").apply(lambda row: \" \".join(row), axis=1)\n",
    "\n",
    "# Memanggil fungsi prediksi\n",
    "results = predict_sentiment(all_data_idi_df['Combined_Text'].tolist())\n",
    "\n",
    "# Menambahkan hasil prediksi ke dataframe asli menggunakan .loc\n",
    "all_data_idi_df.loc[:, 'Label'] = results['sentiment']\n",
    "all_data_idi_df.loc[:, 'Confidence'] = results['confidence']\n",
    "\n",
    "# Menampilkan hasil\n",
    "print(\"\\nSample of labeled data:\")\n",
    "print(all_data_idi_df[['Kualitas Layanan', 'Hubungan dengan OJK', 'Kualitas SDM', 'Saran', 'Label', 'Confidence']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_idi_df.to_csv('data/hasil/main_data_idi.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            Responden      Jenis  \\\n",
      "0                         Responden 1   Asosiasi   \n",
      "1                         Responden 2   Asosiasi   \n",
      "2                         Responden 3   Asosiasi   \n",
      "3                         Responden 4   Asosiasi   \n",
      "4                         Responden 5   Asosiasi   \n",
      "5                         Responden 6   Asosiasi   \n",
      "6                         Responden 7   Asosiasi   \n",
      "7                         Responden 8   Asosiasi   \n",
      "8                         Responden 9   Asosiasi   \n",
      "9             Responden 10 (Dr. Muiz)  Akademisi   \n",
      "10            Responden 11(Dr. Wendy)  Akademisi   \n",
      "11      Responden 12(Prof. Irwan Adi)  Akademisi   \n",
      "12  Responden 13(Prof. Mamduh Hanafi)  Akademisi   \n",
      "\n",
      "                                     Kualitas Layanan  \\\n",
      "0   OJK memiliki layanan syariah yang mana semua p...   \n",
      "1   asosiasi sangat puas atas layanan dan dukungan...   \n",
      "2   Meski perlu ada peningkatan koordinasi dan kom...   \n",
      "3                                    Sudah cukup baik   \n",
      "4                           Masih perlu ditingkatkan.   \n",
      "5                                    Sudah cukup baik   \n",
      "6                                          cukup baik   \n",
      "7                                          cukup baik   \n",
      "8                                          cukup baik   \n",
      "9   Layanan yang diberikan sudah sesuai, hal ini d...   \n",
      "10  Kinerja Otoritas Jasa Keuangan (OJK) dapat dik...   \n",
      "11  Kinerja Otoritas Jasa Keuangan (OJK) dapat dik...   \n",
      "12  Kesannya sekarang kurang agresif dalam menggap...   \n",
      "\n",
      "                                  Hubungan dengan OJK  \\\n",
      "0   Hubungan yang ada mampu menjadi dorongan penti...   \n",
      "1   Setelah pendirian ITSK proses pendirian perusa...   \n",
      "2   SPRINT sudah mumpuni dan memberikan pelayanan ...   \n",
      "3   inovasi layanan masih perlu disediakan untuk m...   \n",
      "4                                         Sandbox 2.0   \n",
      "5   OJK sudah memberikan roadmap pengembangan dan ...   \n",
      "6   sangat memberikan ruang inovasi untuk pembiaya...   \n",
      "7                                            Sudah OK   \n",
      "8                                          cukup baik   \n",
      "9   Diharapkan portal akademik khusus untuk memfas...   \n",
      "10  Universitas Tanjungpura memiliki hubungan baik...   \n",
      "11                                                  -   \n",
      "12   Interaksi dengan perguruan tinggi sudah sanga...   \n",
      "\n",
      "                                         Kualitas SDM  \\\n",
      "0   SDM di OJK sudah kompeten dan memiliki kemampu...   \n",
      "1   Kompetensi pegawai OJK sudah sangat bagus dan ...   \n",
      "2        OJK sudah canggih dengan SDM yang sudah baik   \n",
      "3   yang masih menjadi masalah adalah struktur org...   \n",
      "4                            Birokrasinya menyulitkan   \n",
      "5   Perlu dikembangkan kapasitas dan edukasi liter...   \n",
      "6   Masih kurang jelas, terkadang industri diminta...   \n",
      "7   Masih banyak sdm di OJK daerah yang belum meng...   \n",
      "8   OJK daerah kurang responsif terhadap laporan i...   \n",
      "9   OJK telah menunjukkan kualitas dan kompetensi ...   \n",
      "10  Kompetensi SDM di pusat sangat baik, terutama ...   \n",
      "11  SDM OJK dinilai berpengalaman dan relevan dala...   \n",
      "12  SDM OJK perlu ditingkatkan dengan benchmarking...   \n",
      "\n",
      "                                                Saran  \\\n",
      "0   ke depan perlu adanya kelonggaran dalam regula...   \n",
      "1   Saran kami agar OJK dapat membentuk semacam ta...   \n",
      "2   OJK dapat membentuk regulasi untuk membantu su...   \n",
      "3   memberikan aturan payung untuk mengatur proses...   \n",
      "4   perlu peraturan spesifik untuk sebuah model bi...   \n",
      "5   Diberikan kepastian lamanya waktu untuk mendap...   \n",
      "6            Memudahkan birokrasi dan peningkatan SDM   \n",
      "7                          Peningkatan kompetenis sdm   \n",
      "8   Meningkatkan kapasitas sdm khususnya untuk mod...   \n",
      "9                                                   -   \n",
      "10                                                  -   \n",
      "11  Edukasi terhadap masyarakat secara global perl...   \n",
      "12  Saran untuk dibuat forum untuk menunjang akade...   \n",
      "\n",
      "                                        Combined_Text                Label  \\\n",
      "0   OJK memiliki layanan syariah yang mana semua p...         cukup setuju   \n",
      "1   asosiasi sangat puas atas layanan dan dukungan...         cukup setuju   \n",
      "2   Meski perlu ada peningkatan koordinasi dan kom...         cukup setuju   \n",
      "3   Sudah cukup baik inovasi layanan masih perlu d...         cukup setuju   \n",
      "4   Masih perlu ditingkatkan. Sandbox 2.0 Birokras...         cukup setuju   \n",
      "5   Sudah cukup baik OJK sudah memberikan roadmap ...         cukup setuju   \n",
      "6   cukup baik sangat memberikan ruang inovasi unt...  sangat tidak setuju   \n",
      "7   cukup baik Sudah OK Masih banyak sdm di OJK da...  sangat tidak setuju   \n",
      "8   cukup baik cukup baik OJK daerah kurang respon...  sangat tidak setuju   \n",
      "9   Layanan yang diberikan sudah sesuai, hal ini d...         cukup setuju   \n",
      "10  Kinerja Otoritas Jasa Keuangan (OJK) dapat dik...         cukup setuju   \n",
      "11  Kinerja Otoritas Jasa Keuangan (OJK) dapat dik...         cukup setuju   \n",
      "12  Kesannya sekarang kurang agresif dalam menggap...         cukup setuju   \n",
      "\n",
      "    Confidence  \n",
      "0     0.255784  \n",
      "1     0.248169  \n",
      "2     0.271426  \n",
      "3     0.228766  \n",
      "4     0.224682  \n",
      "5     0.231945  \n",
      "6     0.204839  \n",
      "7     0.196395  \n",
      "8     0.208210  \n",
      "9     0.210667  \n",
      "10    0.236886  \n",
      "11    0.243831  \n",
      "12    0.279341  \n"
     ]
    }
   ],
   "source": [
    "print(all_data_idi_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Menyimpan data ke file CSV\n",
    "# output_file = \"./data/hasil/all_data_idi_ss.csv\"\n",
    "# all_data_idi_df.to_csv(output_file, index=False, sep=';', encoding='utf-8-sig')\n",
    "\n",
    "# print(f\"Data berhasil disimpan ke {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model dan tokenizer berhasil disimpan di ./saved_model\n"
     ]
    }
   ],
   "source": [
    "#SAVE MODEL\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Ganti model_name dengan model IndoBERT\n",
    "model_name = \"indobenchmark/indobert-base-p1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=6  # Sesuaikan jumlah label\n",
    ")\n",
    "\n",
    "# Menyimpan model dan tokenizer ke direktori\n",
    "model_save_path = './saved_model'\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "model.save_pretrained(model_save_path)\n",
    "\n",
    "print(f\"Model dan tokenizer berhasil disimpan di {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model dan tokenizer berhasil dimuat dari ./saved_model\n"
     ]
    }
   ],
   "source": [
    "#LOAD MODEL\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Path ke model yang disimpan\n",
    "model_save_path = './saved_model'\n",
    "\n",
    "# Memuat model dan tokenizer dari path yang telah disimpan\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_save_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_save_path)\n",
    "\n",
    "print(f\"Model dan tokenizer berhasil dimuat dari {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Label  Count\n",
      "0         cukup setuju     10\n",
      "1  sangat tidak setuju      3\n"
     ]
    }
   ],
   "source": [
    "label_counts = all_data_idi_df['Label'].value_counts()\n",
    "\n",
    "label_summary = pd.DataFrame(label_counts).reset_index()\n",
    "label_summary.columns = ['Label', 'Count']\n",
    "print(label_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.makedirs('data/hasil', exist_ok=True)\n",
    "# all_data_idi_df.to_csv('data/hasil/labeled.csv', index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Label  NILAI_SENTIMEN\n",
      "0   setuju               6\n",
      "1   setuju               6\n",
      "2   setuju               6\n",
      "3   setuju               6\n",
      "4   setuju               6\n",
      "5   setuju               6\n",
      "6   setuju               6\n",
      "7   setuju               6\n",
      "8   setuju               6\n",
      "9   setuju               6\n",
      "10  setuju               6\n",
      "11  setuju               6\n",
      "12  setuju               6\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "#VALIDASI\n",
    "\n",
    "file_path = 'data/hasil/main_data_idi.csv'\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv(file_path, delimiter=';')\n",
    "\n",
    "data_copy = data.copy()\n",
    "\n",
    "# Membaca file JSON yang berisi mapping\n",
    "with open('mapping.json', 'r') as file:\n",
    "    text_mapping = json.load(file)\n",
    "\n",
    "\n",
    "# Fungsi untuk menentukan label berdasarkan kata-kata dalam 'Combined_Text'\n",
    "def map_combined_text_to_label(text):\n",
    "    # Iterasi untuk mencari kata kunci dalam text\n",
    "    for keyword, label in text_mapping.items():\n",
    "        if keyword in text.lower():  # Mengabaikan kapitalisasi\n",
    "            return label\n",
    "    return 'setuju'  # Jika tidak ada kata kunci yang ditemukan\n",
    "\n",
    "# Terapkan fungsi ke kolom 'Combined_Text' untuk membuat kolom 'Label'\n",
    "data_copy['New_Label'] = data_copy['Combined_Text'].apply(map_combined_text_to_label)\n",
    "\n",
    "# Tampilkan dataset yang sudah dimodifikasi\n",
    "data_copy['Label'] = data_copy['New_Label'].combine_first(data_copy['Label'])\n",
    "\n",
    "# Simpan dataset yang sudah dimodifikasi\n",
    "# data_copy.to_csv('data/hasil/validated_dataset_new.csv', index=False, sep=';')\n",
    "\n",
    "# Menampilkan statistik label setelah perubahan\n",
    "# label_counts = data_copy['Label'].value_counts()\n",
    "# label_summary = pd.DataFrame(label_counts).reset_index()\n",
    "# label_summary.columns = ['Label', 'Count']\n",
    "\n",
    "label_map = {\n",
    "    1: \"sangat tidak setuju\",\n",
    "    2: \"tidak setuju\",\n",
    "    3: \"kurang setuju\",\n",
    "    4: \"cukup setuju\",\n",
    "    5: \"setuju\",\n",
    "    6: \"sangat setuju\"\n",
    "}\n",
    "\n",
    "reverse_label_map = {v: k for k, v in label_map.items()}  \n",
    "\n",
    "data_copy['Label_Index'] = data_copy['Label'].map(reverse_label_map)\n",
    "\n",
    "def calculate_weight(index):\n",
    "    if index == 1:\n",
    "        return 1\n",
    "    elif 1.1 <= index <= 2.9:\n",
    "        return 2.95\n",
    "    elif 3 <= index <= 3.9:\n",
    "        return 5.9\n",
    "    elif 4 <= index <= 6:\n",
    "        return 6\n",
    "    else:\n",
    "        return None  \n",
    "\n",
    "data_copy['NILAI_SENTIMEN'] = data_copy['Label_Index'].apply(calculate_weight)\n",
    "\n",
    "# print(data_copy)\n",
    "\n",
    "\n",
    "print(data_copy[['Label','NILAI_SENTIMEN']])\n",
    "data_copy.to_csv('data/hasil/main_data_idi.csv', index=False, sep=';')\n",
    "\n",
    "label_counts = data_copy['Label'].value_counts()\n",
    "label_summary = pd.DataFrame(label_counts).reset_index()\n",
    "label_summary.columns = ['Label', 'Count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from nltk.corpus import stopwords\n",
    "# import nltk\n",
    "\n",
    "# # Pastikan Anda sudah mengunduh stopwords dari NLTK\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# # Fungsi untuk membersihkan dan menghilangkan stopwords\n",
    "# def clean_text(text):\n",
    "#     # Ubah ke huruf kecil dan tokenisasi\n",
    "#     text = text.lower()\n",
    "#     # Tokenisasi kata\n",
    "#     words = text.split()\n",
    "#     # Menghilangkan stopwords menggunakan NLTK\n",
    "#     stop_words = set(stopwords.words('indonesian'))  # Sesuaikan dengan bahasa dataset\n",
    "#     words = [word for word in words if word not in stop_words]\n",
    "#     return ' '.join(words)\n",
    "\n",
    "# # Asumsikan `all_data_idi_df` sudah memiliki kolom `Combined_Text`\n",
    "# # Gabungkan teks dari kolom yang relevan jika belum ada\n",
    "# columns_to_process = ['NAMA PIC/RESPONDEN', 'OPEN QUESTION 1', 'OPEN QUESTION 2']\n",
    "# all_data_idi_df['Combined_Text'] = all_data_idi_df[columns_to_process].fillna(\"\").apply(lambda row: \" \".join(row), axis=1)\n",
    "\n",
    "# # Bersihkan teks dari stopwords\n",
    "# all_data_idi_df['Cleaned_Text'] = all_data_idi_df['Combined_Text'].apply(clean_text)\n",
    "\n",
    "# # Inisialisasi CountVectorizer untuk menghitung frekuensi kata\n",
    "# vectorizer = CountVectorizer(max_features=1000, ngram_range=(1, 1))  # Menggunakan unigrams (kata tunggal)\n",
    "# X = vectorizer.fit_transform(all_data_idi_df['Cleaned_Text'])\n",
    "\n",
    "# # Mendapatkan kata-kata dan frekuensinya\n",
    "# word_counts = X.toarray().sum(axis=0)\n",
    "# words = vectorizer.get_feature_names_out()\n",
    "\n",
    "# # Membuat dataframe untuk menampilkan hasil\n",
    "# word_freq_df = pd.DataFrame(zip(words, word_counts), columns=['Word', 'Frequency'])\n",
    "# word_freq_df = word_freq_df.sort_values(by='Frequency', ascending=False)\n",
    "\n",
    "# # Menampilkan kata-kata kunci yang paling sering muncul\n",
    "# print(word_freq_df.head(10))  # Menampilkan 10 kata teratas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.makedirs('data/hasil', exist_ok=True)\n",
    "# all_data_idi_df.to_csv('data/hasil/cleaned.csv', index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_clean_df = pd.read_csv('data/hasil/cleaned.csv', sep=';')\n",
    "# print(data_clean_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Jumlah nilai NaN:\", data_clean_df['Cleaned_Text'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_clean_df = data_clean_df.dropna(subset=['Cleaned_Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Jumlah nilai NaN:\", data_clean_df['Cleaned_Text'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = data_clean_df['Cleaned_Text']\n",
    "# y = data_clean_df['Label']\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# vectorizer = TfidfVectorizer()\n",
    "# X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "# X_test_tfidf = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SVC(kernel='linear') \n",
    "# model.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "# print(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n",
    "# print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}\n",
    "# grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=2)\n",
    "# grid.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_data = [\"Infrastruktur OJK sudah sangat bagus\", \"Saya sudah cukup puas dengan kinerja OJK\", \"OJK sangat bagus\"]\n",
    "# new_data_tfidf = vectorizer.transform(new_data)\n",
    "# prediction = model.predict(new_data_tfidf)\n",
    "# print(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
