{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Necessary Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to C:\\\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data] Downloading package stopwords to C:\\\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to C:\\\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "import nltk\n",
    "os.makedirs(r'C:\\\\nltk_data', exist_ok=True)\n",
    "nltk.data.path.append(r'C:\\\\nltk_data')  \n",
    "nltk.download('punkt_tab', download_dir=r'C:\\\\nltk_data')\n",
    "nltk.download('stopwords', download_dir=r'C:\\\\nltk_data')\n",
    "nltk.download('wordnet', download_dir=r'C:\\\\nltk_data')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Dataset                      ID BIDANG SATKER (AKRONIM)  \\\n",
      "0       Database Internal    DATABASE INTERNAL_23     MS             DPSI   \n",
      "1       Database Internal    DATABASE INTERNAL_24     MS             DPSI   \n",
      "2       Database Internal    DATABASE INTERNAL_25     MS             DPSI   \n",
      "3       Database Internal    DATABASE INTERNAL_26     MS             DPSI   \n",
      "4       Database Internal    DATABASE INTERNAL_28     MS             DPSI   \n",
      "...                   ...                     ...    ...              ...   \n",
      "8242  DATABASE KOJK NO HP  EKSTERNAL_KOJK_HP_5221     KS       KR3 - KOSG   \n",
      "8243  DATABASE KOJK NO HP  EKSTERNAL_KOJK_HP_5222     KS       KR3 - KOSG   \n",
      "8244  DATABASE KOJK NO HP  EKSTERNAL_KOJK_HP_5230     KS       KR3 - KOSG   \n",
      "8245  DATABASE KOJK NO HP  EKSTERNAL_KOJK_HP_5232     KS       KR3 - KOSG   \n",
      "8246  DATABASE KOJK NO HP  EKSTERNAL_KOJK_HP_5235     KS       KR3 - KOSG   \n",
      "\n",
      "      JENIS SURVEI                           TIPE QUESTION  \\\n",
      "0         INTERNAL                             DIRECT DPSI   \n",
      "1         INTERNAL                             DIRECT DPSI   \n",
      "2         INTERNAL                             DIRECT DPSI   \n",
      "3         INTERNAL                             DIRECT DPSI   \n",
      "4         INTERNAL                             DIRECT DPSI   \n",
      "...            ...                                     ...   \n",
      "8242  NON INTERNAL  EDUKASI DAN LITERASI KEUANGAN LANGSUNG   \n",
      "8243  NON INTERNAL  EDUKASI DAN LITERASI KEUANGAN LANGSUNG   \n",
      "8244  NON INTERNAL  EDUKASI DAN LITERASI KEUANGAN LANGSUNG   \n",
      "8245  NON INTERNAL  EDUKASI DAN LITERASI KEUANGAN LANGSUNG   \n",
      "8246  NON INTERNAL  EDUKASI DAN LITERASI KEUANGAN LANGSUNG   \n",
      "\n",
      "     INSTITUSI / PERSEORANGAN/ASAL SATKER       RESPOND  \\\n",
      "0                                    DPAI  SUDAH DI ISI   \n",
      "1                                    DPAI  SUDAH DI ISI   \n",
      "2                                    DPAI  SUDAH DI ISI   \n",
      "3                                    DPAI  SUDAH DI ISI   \n",
      "4                                    DPAI  SUDAH DI ISI   \n",
      "...                                   ...           ...   \n",
      "8242                   Herlin putri utami  SUDAH DI ISI   \n",
      "8243                      SITI PUJI UTAMI  SUDAH DI ISI   \n",
      "8244                      Anna N. Pranoto  SUDAH DI ISI   \n",
      "8245            Nining Nurhasana Pujiarti  SUDAH DI ISI   \n",
      "8246              MARGARETA DIANA SAPUTRI  SUDAH DI ISI   \n",
      "\n",
      "                                      LINK SURVEYMONKEY  \\\n",
      "0       Internal 2 (Direct DOSB & Direct DPSI, 129_173)   \n",
      "1     Internal 1 (Direct DPSI & Indirect DOSB, 253_2...   \n",
      "2     Internal 1 (Direct DPSI & Indirect DOSB, 253_2...   \n",
      "3     Internal 1 (Direct DPSI & Indirect DOSB, 253_2...   \n",
      "4     Internal 1 (Direct DPSI & Indirect DOSB, 253_2...   \n",
      "...                                                 ...   \n",
      "8242                                          KOJK HP 1   \n",
      "8243                                          KOJK HP 1   \n",
      "8244                                          KOJK HP 1   \n",
      "8245                                          KOJK HP 1   \n",
      "8246                                          KOJK HP 1   \n",
      "\n",
      "                  WEBLINK SURVEYMONKEY  ... INTEREST KATEGORI        DATE  \\\n",
      "0     http://uns.id/SurveyInternalOJK2  ...     HIGH   PLAYER  12/02/2024   \n",
      "1     http://uns.id/SurveyInternalOJK1  ...     HIGH   PLAYER  12/09/2024   \n",
      "2     http://uns.id/SurveyInternalOJK1  ...     HIGH   PLAYER  12/16/2024   \n",
      "3     http://uns.id/SurveyInternalOJK1  ...     HIGH   PLAYER  12/09/2024   \n",
      "4     http://uns.id/SurveyInternalOJK1  ...     HIGH   PLAYER  12/12/2024   \n",
      "...                                ...  ...      ...      ...         ...   \n",
      "8242                               NaN  ...     HIGH   PLAYER         NaN   \n",
      "8243                               NaN  ...     HIGH   PLAYER         NaN   \n",
      "8244                               NaN  ...     HIGH   PLAYER         NaN   \n",
      "8245                               NaN  ...     HIGH   PLAYER         NaN   \n",
      "8246                               NaN  ...     HIGH   PLAYER         NaN   \n",
      "\n",
      "     Unnamed: 23 RESOURCE PERCEPTION PERFORMANCE DELIVERY   OS PENTING  \\\n",
      "0            NaN         5,333333333          5,285714286            6   \n",
      "1            NaN         5,666666667                    6            6   \n",
      "2            NaN         4,666666667          4,571428571            6   \n",
      "3            NaN                   5                    5            6   \n",
      "4            NaN                   6          5,285714286            6   \n",
      "...          ...                 ...                  ...          ...   \n",
      "8242         NaN                4,75                  4,7            4   \n",
      "8243         NaN                   5                  5,3            5   \n",
      "8244         NaN                 NaN                  NaN          NaN   \n",
      "8245         NaN                5,25                  4,8  5,333333333   \n",
      "8246         NaN                   6                    6            6   \n",
      "\n",
      "     OS PUAS                                    OPEN QUESTION 1  \\\n",
      "0          5  Sdh sangat bagus hanya banyak terkendala krn k...   \n",
      "1          6                                        sangat baik   \n",
      "2          4  Cukup puas. Perlu di tingkatkan lagi terkait b...   \n",
      "3          5                            sudah baik, pertahankan   \n",
      "4          6                     Sejauh ini baik dan responsif.   \n",
      "...      ...                                                ...   \n",
      "8242       4  Sangat edukatif dan dapat mengedukasi mahasisw...   \n",
      "8243       5  OJK sangat membantu dalam memberi edukasi keua...   \n",
      "8244     NaN                                                NaN   \n",
      "8245       5  Menurut saya, kegiatan edukasi dan literasi ke...   \n",
      "8246       6  Pelaksanaan kegiatan edukasi dan literasi keua...   \n",
      "\n",
      "                                        OPEN QUESTION 2  \n",
      "0     fungsi pengelolaan informasi harus imbang anta...  \n",
      "1                                                     -  \n",
      "2     Agar dibuat single sign on, pusat database ata...  \n",
      "3     sudah cukup baik pertahankan kinerja yang suda...  \n",
      "4     Agar terdapat no wa helpdesk per masing-masing...  \n",
      "...                                                 ...  \n",
      "8242  Sarannya, lebih sering mengadakan atau sharing...  \n",
      "8243                                                  -  \n",
      "8244                                                NaN  \n",
      "8245  Saran dari saya, agar kegiatan tersebut setiap...  \n",
      "8246  Saran untuk meningkatkan kegiatan edukasi dan ...  \n",
      "\n",
      "[8247 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "def read_database_result_csv(file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, sep=';', encoding='utf-8')\n",
    "    except UnicodeDecodeError as e1:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, sep=';', encoding='latin1')\n",
    "        except UnicodeDecodeError as e2:\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, sep=';', encoding='cp1252')\n",
    "            except Exception as e3:\n",
    "                print(f\"Gagal membaca file {file_path}: {e1} | {e2} | {e3}\")\n",
    "                return pd.DataFrame()  \n",
    "    return df\n",
    "\n",
    "file_path = 'data/main_data_19.csv'\n",
    "database_result_df = read_database_result_csv(file_path)\n",
    "\n",
    "print(database_result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_order = [\n",
    "    'ID',\n",
    "    'BIDANG',\n",
    "    'SATKER (AKRONIM)',\n",
    "    'JENIS SURVEI',\n",
    "    'TIPE QUESTION',\n",
    "    'INSTITUSI / PERSEORANGAN/ASAL SATKER',\n",
    "    'RESPOND',\n",
    "    'LINK SURVEYMONKEY',\n",
    "    'TOKEN',\n",
    "    'NAMA PIC/RESPONDEN',\n",
    "    'JABATAN/PROFESI/LVEL DI OJK',\n",
    "    'KONTAK',\n",
    "    'FUNGSI YANG DINILAI',\n",
    "    'DIRECT / INDIRECT',\n",
    "    'JENIS STAKEHOLDERS',\n",
    "    'RELASI RESPONDEN DENGAN SATKER',\n",
    "    'POWER',\n",
    "    'INTEREST',\n",
    "    'KATEGORI',\n",
    "    'Dataset',\n",
    "    'RESOURCE PERCEPTION',\n",
    "    'PERFORMANCE DELIVERY',\n",
    "    'OPEN QUESTION 1',\n",
    "    'OPEN QUESTION 2'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          ID BIDANG SATKER (AKRONIM)  JENIS SURVEI  \\\n",
      "0       DATABASE INTERNAL_23     MS             DPSI      INTERNAL   \n",
      "1       DATABASE INTERNAL_24     MS             DPSI      INTERNAL   \n",
      "2       DATABASE INTERNAL_25     MS             DPSI      INTERNAL   \n",
      "3       DATABASE INTERNAL_26     MS             DPSI      INTERNAL   \n",
      "4       DATABASE INTERNAL_28     MS             DPSI      INTERNAL   \n",
      "...                      ...    ...              ...           ...   \n",
      "8242  EKSTERNAL_KOJK_HP_5221     KS       KR3 - KOSG  NON INTERNAL   \n",
      "8243  EKSTERNAL_KOJK_HP_5222     KS       KR3 - KOSG  NON INTERNAL   \n",
      "8244  EKSTERNAL_KOJK_HP_5230     KS       KR3 - KOSG  NON INTERNAL   \n",
      "8245  EKSTERNAL_KOJK_HP_5232     KS       KR3 - KOSG  NON INTERNAL   \n",
      "8246  EKSTERNAL_KOJK_HP_5235     KS       KR3 - KOSG  NON INTERNAL   \n",
      "\n",
      "                               TIPE QUESTION  \\\n",
      "0                                DIRECT DPSI   \n",
      "1                                DIRECT DPSI   \n",
      "2                                DIRECT DPSI   \n",
      "3                                DIRECT DPSI   \n",
      "4                                DIRECT DPSI   \n",
      "...                                      ...   \n",
      "8242  EDUKASI DAN LITERASI KEUANGAN LANGSUNG   \n",
      "8243  EDUKASI DAN LITERASI KEUANGAN LANGSUNG   \n",
      "8244  EDUKASI DAN LITERASI KEUANGAN LANGSUNG   \n",
      "8245  EDUKASI DAN LITERASI KEUANGAN LANGSUNG   \n",
      "8246  EDUKASI DAN LITERASI KEUANGAN LANGSUNG   \n",
      "\n",
      "     INSTITUSI / PERSEORANGAN/ASAL SATKER       RESPOND  \\\n",
      "0                                    DPAI  SUDAH DI ISI   \n",
      "1                                    DPAI  SUDAH DI ISI   \n",
      "2                                    DPAI  SUDAH DI ISI   \n",
      "3                                    DPAI  SUDAH DI ISI   \n",
      "4                                    DPAI  SUDAH DI ISI   \n",
      "...                                   ...           ...   \n",
      "8242                   Herlin putri utami  SUDAH DI ISI   \n",
      "8243                      SITI PUJI UTAMI  SUDAH DI ISI   \n",
      "8244                      Anna N. Pranoto  SUDAH DI ISI   \n",
      "8245            Nining Nurhasana Pujiarti  SUDAH DI ISI   \n",
      "8246              MARGARETA DIANA SAPUTRI  SUDAH DI ISI   \n",
      "\n",
      "                                      LINK SURVEYMONKEY       TOKEN  \\\n",
      "0       Internal 2 (Direct DOSB & Direct DPSI, 129_173)  Token 0128   \n",
      "1     Internal 1 (Direct DPSI & Indirect DOSB, 253_2...  Token 0252   \n",
      "2     Internal 1 (Direct DPSI & Indirect DOSB, 253_2...  Token 0252   \n",
      "3     Internal 1 (Direct DPSI & Indirect DOSB, 253_2...  Token 0252   \n",
      "4     Internal 1 (Direct DPSI & Indirect DOSB, 253_2...  Token 0252   \n",
      "...                                                 ...         ...   \n",
      "8242                                          KOJK HP 1           4   \n",
      "8243                                          KOJK HP 1           4   \n",
      "8244                                          KOJK HP 1           4   \n",
      "8245                                          KOJK HP 1           4   \n",
      "8246                                          KOJK HP 1           4   \n",
      "\n",
      "                    NAMA PIC/RESPONDEN  ...            JENIS STAKEHOLDERS  \\\n",
      "0          Albertus Widjono Pratijakso  ...                   PEGAWAI OJK   \n",
      "1     Glendy Mario Constantine Walujan  ...                   PEGAWAI OJK   \n",
      "2                        Yulia Nurlita  ...                   PEGAWAI OJK   \n",
      "3                      Tengku Faradina  ...                   PEGAWAI OJK   \n",
      "4                     Sefrina Widyanti  ...                   PEGAWAI OJK   \n",
      "...                                ...  ...                           ...   \n",
      "8242                Herlin putri utami  ...  UNIVERSITAS/PERGURUAN TINGGI   \n",
      "8243                   SITI PUJI UTAMI  ...  UNIVERSITAS/PERGURUAN TINGGI   \n",
      "8244                   Anna N. Pranoto  ...  UNIVERSITAS/PERGURUAN TINGGI   \n",
      "8245         Nining Nurhasana Pujiarti  ...  UNIVERSITAS/PERGURUAN TINGGI   \n",
      "8246           MARGARETA DIANA SAPUTRI  ...  UNIVERSITAS/PERGURUAN TINGGI   \n",
      "\n",
      "                RELASI RESPONDEN DENGAN SATKER POWER INTEREST KATEGORI  \\\n",
      "0     PENERIMA HELPDESK & LAYANAN DPSI LAINNYA  HIGH     HIGH   PLAYER   \n",
      "1     PENERIMA HELPDESK & LAYANAN DPSI LAINNYA  HIGH     HIGH   PLAYER   \n",
      "2     PENERIMA HELPDESK & LAYANAN DPSI LAINNYA  HIGH     HIGH   PLAYER   \n",
      "3     PENERIMA HELPDESK & LAYANAN DPSI LAINNYA  HIGH     HIGH   PLAYER   \n",
      "4     PENERIMA HELPDESK & LAYANAN DPSI LAINNYA  HIGH     HIGH   PLAYER   \n",
      "...                                        ...   ...      ...      ...   \n",
      "8242              UNIVERSITAS/PERGURUAN TINGGI  HIGH     HIGH   PLAYER   \n",
      "8243              UNIVERSITAS/PERGURUAN TINGGI  HIGH     HIGH   PLAYER   \n",
      "8244              UNIVERSITAS/PERGURUAN TINGGI  HIGH     HIGH   PLAYER   \n",
      "8245              UNIVERSITAS/PERGURUAN TINGGI  HIGH     HIGH   PLAYER   \n",
      "8246              UNIVERSITAS/PERGURUAN TINGGI  HIGH     HIGH   PLAYER   \n",
      "\n",
      "                  Dataset RESOURCE PERCEPTION PERFORMANCE DELIVERY  \\\n",
      "0       Database Internal         5,333333333          5,285714286   \n",
      "1       Database Internal         5,666666667                    6   \n",
      "2       Database Internal         4,666666667          4,571428571   \n",
      "3       Database Internal                   5                    5   \n",
      "4       Database Internal                   6          5,285714286   \n",
      "...                   ...                 ...                  ...   \n",
      "8242  DATABASE KOJK NO HP                4,75                  4,7   \n",
      "8243  DATABASE KOJK NO HP                   5                  5,3   \n",
      "8244  DATABASE KOJK NO HP                 NaN                  NaN   \n",
      "8245  DATABASE KOJK NO HP                5,25                  4,8   \n",
      "8246  DATABASE KOJK NO HP                   6                    6   \n",
      "\n",
      "                                        OPEN QUESTION 1  \\\n",
      "0     Sdh sangat bagus hanya banyak terkendala krn k...   \n",
      "1                                           sangat baik   \n",
      "2     Cukup puas. Perlu di tingkatkan lagi terkait b...   \n",
      "3                               sudah baik, pertahankan   \n",
      "4                        Sejauh ini baik dan responsif.   \n",
      "...                                                 ...   \n",
      "8242  Sangat edukatif dan dapat mengedukasi mahasisw...   \n",
      "8243  OJK sangat membantu dalam memberi edukasi keua...   \n",
      "8244                                                NaN   \n",
      "8245  Menurut saya, kegiatan edukasi dan literasi ke...   \n",
      "8246  Pelaksanaan kegiatan edukasi dan literasi keua...   \n",
      "\n",
      "                                        OPEN QUESTION 2  \n",
      "0     fungsi pengelolaan informasi harus imbang anta...  \n",
      "1                                                     -  \n",
      "2     Agar dibuat single sign on, pusat database ata...  \n",
      "3     sudah cukup baik pertahankan kinerja yang suda...  \n",
      "4     Agar terdapat no wa helpdesk per masing-masing...  \n",
      "...                                                 ...  \n",
      "8242  Sarannya, lebih sering mengadakan atau sharing...  \n",
      "8243                                                  -  \n",
      "8244                                                NaN  \n",
      "8245  Saran dari saya, agar kegiatan tersebut setiap...  \n",
      "8246  Saran untuk meningkatkan kegiatan edukasi dan ...  \n",
      "\n",
      "[8247 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "all_data_idi_df = database_result_df[columns_order]\n",
    "print(all_data_idi_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID                                         0\n",
      "BIDANG                                     0\n",
      "SATKER (AKRONIM)                           0\n",
      "JENIS SURVEI                               0\n",
      "TIPE QUESTION                              0\n",
      "INSTITUSI / PERSEORANGAN/ASAL SATKER       3\n",
      "RESPOND                                    0\n",
      "LINK SURVEYMONKEY                          0\n",
      "TOKEN                                      0\n",
      "NAMA PIC/RESPONDEN                        14\n",
      "JABATAN/PROFESI/LVEL DI OJK              239\n",
      "KONTAK                                   185\n",
      "FUNGSI YANG DINILAI                        0\n",
      "DIRECT / INDIRECT                          0\n",
      "JENIS STAKEHOLDERS                         3\n",
      "RELASI RESPONDEN DENGAN SATKER           941\n",
      "POWER                                      0\n",
      "INTEREST                                   0\n",
      "KATEGORI                                   0\n",
      "Dataset                                    0\n",
      "RESOURCE PERCEPTION                     3793\n",
      "PERFORMANCE DELIVERY                    3793\n",
      "OPEN QUESTION 1                         1318\n",
      "OPEN QUESTION 2                         1367\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(all_data_idi_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID                                      0\n",
      "BIDANG                                  0\n",
      "SATKER (AKRONIM)                        0\n",
      "JENIS SURVEI                            0\n",
      "TIPE QUESTION                           0\n",
      "INSTITUSI / PERSEORANGAN/ASAL SATKER    0\n",
      "RESPOND                                 0\n",
      "LINK SURVEYMONKEY                       0\n",
      "TOKEN                                   0\n",
      "NAMA PIC/RESPONDEN                      0\n",
      "JABATAN/PROFESI/LVEL DI OJK             0\n",
      "KONTAK                                  0\n",
      "FUNGSI YANG DINILAI                     0\n",
      "DIRECT / INDIRECT                       0\n",
      "JENIS STAKEHOLDERS                      0\n",
      "RELASI RESPONDEN DENGAN SATKER          0\n",
      "POWER                                   0\n",
      "INTEREST                                0\n",
      "KATEGORI                                0\n",
      "Dataset                                 0\n",
      "RESOURCE PERCEPTION                     0\n",
      "PERFORMANCE DELIVERY                    0\n",
      "OPEN QUESTION 1                         0\n",
      "OPEN QUESTION 2                         0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_14688\\1738321216.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  all_data_idi_df.fillna(\"-\", inplace=True)\n"
     ]
    }
   ],
   "source": [
    "all_data_idi_df.fillna(\"-\", inplace=True)\n",
    "print(all_data_idi_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>BIDANG</th>\n",
       "      <th>SATKER (AKRONIM)</th>\n",
       "      <th>JENIS SURVEI</th>\n",
       "      <th>TIPE QUESTION</th>\n",
       "      <th>INSTITUSI / PERSEORANGAN/ASAL SATKER</th>\n",
       "      <th>RESPOND</th>\n",
       "      <th>LINK SURVEYMONKEY</th>\n",
       "      <th>TOKEN</th>\n",
       "      <th>NAMA PIC/RESPONDEN</th>\n",
       "      <th>...</th>\n",
       "      <th>JENIS STAKEHOLDERS</th>\n",
       "      <th>RELASI RESPONDEN DENGAN SATKER</th>\n",
       "      <th>POWER</th>\n",
       "      <th>INTEREST</th>\n",
       "      <th>KATEGORI</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>RESOURCE PERCEPTION</th>\n",
       "      <th>PERFORMANCE DELIVERY</th>\n",
       "      <th>OPEN QUESTION 1</th>\n",
       "      <th>OPEN QUESTION 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8247</td>\n",
       "      <td>8247</td>\n",
       "      <td>8247</td>\n",
       "      <td>8247</td>\n",
       "      <td>8247</td>\n",
       "      <td>8247</td>\n",
       "      <td>8247</td>\n",
       "      <td>8247</td>\n",
       "      <td>8247</td>\n",
       "      <td>8247</td>\n",
       "      <td>...</td>\n",
       "      <td>8247</td>\n",
       "      <td>8247</td>\n",
       "      <td>8247</td>\n",
       "      <td>8247</td>\n",
       "      <td>8247</td>\n",
       "      <td>8247</td>\n",
       "      <td>8247</td>\n",
       "      <td>8247</td>\n",
       "      <td>8247</td>\n",
       "      <td>8247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>8247</td>\n",
       "      <td>9</td>\n",
       "      <td>56</td>\n",
       "      <td>2</td>\n",
       "      <td>73</td>\n",
       "      <td>1437</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>237</td>\n",
       "      <td>4040</td>\n",
       "      <td>...</td>\n",
       "      <td>183</td>\n",
       "      <td>238</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>26</td>\n",
       "      <td>90</td>\n",
       "      <td>3269</td>\n",
       "      <td>4538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>EKSTERNAL_KOJK_HP_5235</td>\n",
       "      <td>MS</td>\n",
       "      <td>DPSI</td>\n",
       "      <td>INTERNAL</td>\n",
       "      <td>DIRECT DPSI</td>\n",
       "      <td>DPW1</td>\n",
       "      <td>SUDAH DI ISI</td>\n",
       "      <td>Internal 1 (Direct DPSI &amp; Indirect DOSB, 253_2...</td>\n",
       "      <td>Token 0252</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>PEGAWAI OJK</td>\n",
       "      <td>PENERIMA HELPDESK &amp; LAYANAN DPSI LAINNYA</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>PLAYER</td>\n",
       "      <td>Database Internal</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>5362</td>\n",
       "      <td>2474</td>\n",
       "      <td>5940</td>\n",
       "      <td>2458</td>\n",
       "      <td>205</td>\n",
       "      <td>8247</td>\n",
       "      <td>3334</td>\n",
       "      <td>3334</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>5919</td>\n",
       "      <td>2458</td>\n",
       "      <td>5364</td>\n",
       "      <td>7722</td>\n",
       "      <td>5117</td>\n",
       "      <td>4106</td>\n",
       "      <td>3793</td>\n",
       "      <td>3793</td>\n",
       "      <td>2446</td>\n",
       "      <td>2822</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            ID BIDANG SATKER (AKRONIM) JENIS SURVEI  \\\n",
       "count                     8247   8247             8247         8247   \n",
       "unique                    8247      9               56            2   \n",
       "top     EKSTERNAL_KOJK_HP_5235     MS             DPSI     INTERNAL   \n",
       "freq                         1   5362             2474         5940   \n",
       "\n",
       "       TIPE QUESTION INSTITUSI / PERSEORANGAN/ASAL SATKER       RESPOND  \\\n",
       "count           8247                                 8247          8247   \n",
       "unique            73                                 1437             1   \n",
       "top      DIRECT DPSI                                 DPW1  SUDAH DI ISI   \n",
       "freq            2458                                  205          8247   \n",
       "\n",
       "                                        LINK SURVEYMONKEY       TOKEN  \\\n",
       "count                                                8247        8247   \n",
       "unique                                                 25         237   \n",
       "top     Internal 1 (Direct DPSI & Indirect DOSB, 253_2...  Token 0252   \n",
       "freq                                                 3334        3334   \n",
       "\n",
       "       NAMA PIC/RESPONDEN  ... JENIS STAKEHOLDERS  \\\n",
       "count                8247  ...               8247   \n",
       "unique               4040  ...                183   \n",
       "top                     -  ...        PEGAWAI OJK   \n",
       "freq                   14  ...               5919   \n",
       "\n",
       "                  RELASI RESPONDEN DENGAN SATKER POWER INTEREST KATEGORI  \\\n",
       "count                                       8247  8247     8247     8247   \n",
       "unique                                       238     3        3        4   \n",
       "top     PENERIMA HELPDESK & LAYANAN DPSI LAINNYA  HIGH     HIGH   PLAYER   \n",
       "freq                                        2458  5364     7722     5117   \n",
       "\n",
       "                  Dataset RESOURCE PERCEPTION PERFORMANCE DELIVERY  \\\n",
       "count                8247                8247                 8247   \n",
       "unique                  7                  26                   90   \n",
       "top     Database Internal                   -                    -   \n",
       "freq                 4106                3793                 3793   \n",
       "\n",
       "       OPEN QUESTION 1 OPEN QUESTION 2  \n",
       "count             8247            8247  \n",
       "unique            3269            4538  \n",
       "top                  -               -  \n",
       "freq              2446            2822  \n",
       "\n",
       "[4 rows x 24 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_idi_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+|\\@\\w+|\\#|\\d+|[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    words = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('indonesian'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    cleaned_text = ' '.join(words)\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Menggunakan Model: indobenchmark/indobert-base-p1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Analyzing Sentiments with indobenchmark/indobert-base-p1:   2%|▏         | 136/8247 [00:12<12:29, 10.83it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 108\u001b[0m\n\u001b[0;32m    105\u001b[0m df_copy \u001b[38;5;241m=\u001b[39m all_data_idi_df\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# Jalankan analisis sentimen\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m result_df \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_sentiment_with_custom_mapping\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf_copy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns_to_process\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m# Tampilkan distribusi label\u001b[39;00m\n\u001b[0;32m    115\u001b[0m label_counts \u001b[38;5;241m=\u001b[39m result_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts()\n",
      "Cell \u001b[1;32mIn[13], line 72\u001b[0m, in \u001b[0;36mpredict_sentiment_with_custom_mapping\u001b[1;34m(df, text_columns, model_name)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m tqdm(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m], desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnalyzing Sentiments with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     71\u001b[0m     processed_text \u001b[38;5;241m=\u001b[39m preprocess_text(text)\n\u001b[1;32m---> 72\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_single_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(result)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Konversi hasil ke DataFrame\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[13], line 52\u001b[0m, in \u001b[0;36mpredict_sentiment_with_custom_mapping.<locals>.predict_single_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     50\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 52\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoded\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoded\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(outputs\u001b[38;5;241m.\u001b[39mlogits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     54\u001b[0m     predicted_label \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(predictions, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1668\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1660\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1661\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1662\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1663\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1664\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1665\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1666\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1668\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1674\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1675\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1676\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1678\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1680\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   1682\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1142\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m   1138\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m   1139\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m   1140\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m-> 1142\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1154\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1155\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:695\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    684\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    685\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    686\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    692\u001b[0m         output_attentions,\n\u001b[0;32m    693\u001b[0m     )\n\u001b[0;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 695\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:585\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    574\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    575\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    582\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    583\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    584\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 585\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    592\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    594\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:524\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    507\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    513\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    514\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    515\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself(\n\u001b[0;32m    516\u001b[0m         hidden_states,\n\u001b[0;32m    517\u001b[0m         attention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    522\u001b[0m         output_attentions,\n\u001b[0;32m    523\u001b[0m     )\n\u001b[1;32m--> 524\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mself_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    525\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n\u001b[0;32m    526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:466\u001b[0m, in \u001b[0;36mBertSelfOutput.forward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 466\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    467\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m    468\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def predict_sentiment_with_custom_mapping(df, text_columns, model_name):\n",
    "    # Inisialisasi tokenizer dan model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=6  # Sesuaikan dengan jumlah label Anda\n",
    "    )\n",
    "    \n",
    "    # Definisi label mapping\n",
    "    label_map = {\n",
    "        0: \"sangat tidak setuju\",\n",
    "        1: \"tidak setuju\",\n",
    "        2: \"kurang setuju\",\n",
    "        3: \"cukup setuju\", \n",
    "        4: \"setuju\",\n",
    "        5: \"sangat setuju\"\n",
    "    }\n",
    "    \n",
    "    # Fungsi preprocessing khusus\n",
    "    def preprocess_text(text):\n",
    "        # Jika teks adalah \"-\", kembalikan \"-\"\n",
    "        if isinstance(text, str) and text.strip() == \"-\":\n",
    "            return \"-\"\n",
    "        # Proses teks normal\n",
    "        return text.strip().lower() if isinstance(text, str) else \"\"\n",
    "\n",
    "    # Gabungkan kolom teks\n",
    "    df['Text'] = df[text_columns].fillna(\"-\").apply(lambda row: \" \".join(row), axis=1)\n",
    "    \n",
    "    # Fungsi prediksi sentimen\n",
    "    def predict_single_text(text):\n",
    "        # Jika teks adalah \"-\", kembalikan \"setuju\"\n",
    "        if text == \"-\":\n",
    "            return {\n",
    "                'text': text,\n",
    "                'sentiment': \"setuju\",\n",
    "                'confidence': 1.0\n",
    "            }\n",
    "        \n",
    "        # Tokenisasi\n",
    "        encoded = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Prediksi\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(encoded[\"input_ids\"], attention_mask=encoded[\"attention_mask\"])\n",
    "            predictions = F.softmax(outputs.logits, dim=1)\n",
    "            predicted_label = torch.argmax(predictions, dim=1).item()\n",
    "            \n",
    "            # Ambil confidence score\n",
    "            confidence = predictions[0][predicted_label].item()\n",
    "            \n",
    "            # Mapping label\n",
    "            sentiment = label_map.get(predicted_label, \"unknown\")\n",
    "            \n",
    "            return {\n",
    "                'text': text,\n",
    "                'sentiment': sentiment,\n",
    "                'confidence': confidence\n",
    "            }\n",
    "    \n",
    "    # Proses sentimen untuk seluruh dataset\n",
    "    results = []\n",
    "    for text in tqdm(df['Text'], desc=f\"Analyzing Sentiments with {model_name}\"):\n",
    "        processed_text = preprocess_text(text)\n",
    "        result = predict_single_text(processed_text)\n",
    "        results.append(result)\n",
    "    \n",
    "    # Konversi hasil ke DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Update DataFrame asli\n",
    "    df['Label'] = results_df['sentiment']\n",
    "    df['Confidence'] = results_df['confidence']\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Model-model untuk diuji\n",
    "alternative_models = [\n",
    "    \"indobenchmark/indobert-base-p1\",\n",
    "    \"bert-base-multilingual-uncased\",\n",
    "    \"indobenchmark/indobert-base-p2\", \n",
    "    \"NLP-Cube-Lab/indonesian-sentiment-bert\",\n",
    "    \"kambaa/indonesian-sentiment-analysis-bert\"\n",
    "]\n",
    "\n",
    "# Kolom untuk diproses\n",
    "columns_to_process = ['OPEN QUESTION 1', 'OPEN QUESTION 2']\n",
    "\n",
    "# Simpan hasil untuk setiap model\n",
    "results_summary = {}\n",
    "\n",
    "# Uji coba setiap model\n",
    "for model_name in alternative_models:\n",
    "    try:\n",
    "        print(f\"\\n--- Menggunakan Model: {model_name} ---\")\n",
    "        \n",
    "        # Buat salinan dataframe untuk setiap iterasi\n",
    "        df_copy = all_data_idi_df.copy()\n",
    "        \n",
    "        # Jalankan analisis sentimen\n",
    "        result_df = predict_sentiment_with_custom_mapping(\n",
    "            df_copy, \n",
    "            columns_to_process, \n",
    "            model_name\n",
    "        )\n",
    "        \n",
    "        # Tampilkan distribusi label\n",
    "        label_counts = result_df['Label'].value_counts()\n",
    "        print(\"\\nDistribusi Label Sentimen:\")\n",
    "        print(label_counts)\n",
    "        \n",
    "        # Simpan hasil\n",
    "        output_filename = f'data/hasil/sentiment_{model_name.replace(\"/\", \"_\")}_custom.csv'\n",
    "        result_df.to_csv(output_filename, index=False, sep=';')\n",
    "        print(f\"\\nHasil disimpan di: {output_filename}\")\n",
    "        \n",
    "        # Simpan ringkasan\n",
    "        results_summary[model_name] = {\n",
    "            'label_distribution': label_counts.to_dict(),\n",
    "            'output_file': output_filename\n",
    "        }\n",
    "        \n",
    "        # Tampilkan beberapa sampel\n",
    "        print(\"\\nSampel Hasil:\")\n",
    "        print(result_df[['OPEN QUESTION 1', 'OPEN QUESTION 2', 'Label', 'Confidence']].head())\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error dengan model {model_name}: {e}\")\n",
    "\n",
    "# Simpan ringkasan hasil\n",
    "import json\n",
    "with open('data/hasil/sentiment_models_summary.json', 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "print(\"\\n--- Ringkasan Hasil Tersimpan ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_idi_df.to_csv('data/hasil/main_data_19_OQ2.csv', index=False, sep=';')\n",
    "print(all_data_idi_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cek lagi\n",
    "label_counts = all_data_idi_df['Label'].value_counts()\n",
    "\n",
    "label_summary = pd.DataFrame(label_counts).reset_index()\n",
    "label_summary.columns = ['Label', 'Count']\n",
    "print(label_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
